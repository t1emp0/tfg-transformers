{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "\n",
    "import random\n",
    "from collections import Counter\n",
    "from nltk import tokenize\n",
    "from unicodedata import normalize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the contents of the collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 33 collections\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "\n",
    "GEC_folder = \"data/EnciclopediaCat\"\n",
    "csv_collections = [f\"{GEC_folder}/{collection}\" for collection in listdir(GEC_folder)]\n",
    "print(\"Loaded\", len(csv_collections), \"collections\")\n",
    "\n",
    "id_to_collection = {\n",
    "    id: collection.split(\"/\")[2][:-4] for (id, collection) in enumerate(csv_collections)\n",
    "}\n",
    "collection_to_id = {\n",
    "    collection.split(\"/\")[2][:-4]: id for (id, collection) in enumerate(csv_collections)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles on each  collection: [2549, 45, 277, 150, 26, 439, 133, 138, 8380, 1110, 46, 16, 12183, 678, 1987, 13524, 347, 109875, 498, 1889, 38, 138, 1851, 871, 3712, 18, 98, 9673, 145, 178, 122, 52, 0]\n",
      "\n",
      "Mean number of articles per collection: 5187.45\n",
      "Std desv of articles per collection: 18840.71\n"
     ]
    }
   ],
   "source": [
    "# Output naming convention: parsed[ collection[ article( title, text ) ] ]\n",
    "parsed = []\n",
    "previous_lens = []\n",
    "for collection in csv_collections:\n",
    "    df = pd.read_csv(collection, sep=\";\", dtype=\"unicode\")\n",
    "\n",
    "    previous_lens.append(df.shape[0])\n",
    "    df = df[[\"title\", \"Body\"]].dropna()\n",
    "    df = df[df[\"title\"] != \"Crèdits\"]\n",
    "\n",
    "    articles = list(zip(df[\"title\"], df[\"Body\"]))\n",
    "    parsed.append(articles)\n",
    "\n",
    "\n",
    "lens = [len(t) for t in parsed]\n",
    "print(\"Number of articles on each  collection:\", lens)\n",
    "print()\n",
    "print(\"Mean number of articles per collection: %.2f\" % np.mean(lens))\n",
    "print(\"Std desv of articles per collection: %.2f\" % np.std(lens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graphs/GEC_EDA_collection_articles.html'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(zip(previous_lens, lens)).rename(\n",
    "    index=id_to_collection, columns={0: \"Raw\", 1: \"Filtered\"}\n",
    ")\n",
    "df[\"nans\"] = df[\"Raw\"] - df[\"Filtered\"]\n",
    "fig = px.bar(\n",
    "    df, barmode=\"group\", log_y=True, labels={\"index\": \"Collection\", \"value\": \"Articles\"}\n",
    ")\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.update_layout(\n",
    "    legend_title=\"\", legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.99)\n",
    ")\n",
    "\n",
    "avg = df[\"Filtered\"].mean()\n",
    "fig.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=-0.5,\n",
    "    x1=len(lens),\n",
    "    y0=avg,\n",
    "    y1=avg,\n",
    "    line=dict(color=\"Red\",),\n",
    "    xref=\"x\",\n",
    "    yref=\"y\",\n",
    ")\n",
    "# fig.show()\n",
    "\n",
    "plotly.offline.plot(fig, filename=\"graphs/GEC_EDA_collection_articles.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    171186.000000\n",
      "mean        341.500765\n",
      "std        1388.989449\n",
      "min           1.000000\n",
      "25%          34.000000\n",
      "50%          78.000000\n",
      "75%         178.000000\n",
      "max       61221.000000\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'graphs/GEC_EDA_words_per_article.html'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Number of words per article\"\"\"\n",
    "texts_words = pd.Series(\n",
    "    [len(text.split(\" \")) for collection in parsed for (_, text) in collection]\n",
    ")\n",
    "print(texts_words.describe())\n",
    "\n",
    "fig = px.histogram(texts_words, labels={\"value\": \"Words per article\"}, log_y=True)\n",
    "avg = np.mean(texts_words)\n",
    "fig.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=-0.5,\n",
    "    x1=max(texts_words),\n",
    "    y0=avg,\n",
    "    y1=avg,\n",
    "    line=dict(color=\"Red\",),\n",
    "    xref=\"x\",\n",
    "    yref=\"y\",\n",
    ")\n",
    "fig.update_layout(showlegend=False, font_size=15)\n",
    "# fig.show()\n",
    "\n",
    "plotly.offline.plot(fig, filename=\"graphs/GEC_EDA_words_per_article.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Texts must be normalized because some contain some badly formatted chars\"\"\"\n",
    "\"\"\"Also, some replacements must be made in order to adapt it for the tokenizer\"\"\"\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = (\n",
    "        normalize(\"NFKC\", text)\n",
    "        .replace(\"\\t\", \"\")\n",
    "        .replace(\"\\n\\n\", \"\\n\")\n",
    "        .replace(\"L’\", \"L'\")\n",
    "        .replace(\"l’\", \"l'\")\n",
    "        .replace(\"S’\", \"S'\")\n",
    "        .replace(\"s’\", \"s'\")\n",
    "        .replace(\"D’\", \"D'\")\n",
    "        .replace(\"d’\", \"d'\")\n",
    "        .replace(\"N’\", \"N'\")\n",
    "        .replace(\"n’\", \"n'\")\n",
    "        .replace(\"e’\", \"e'\")\n",
    "    )\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171186\n"
     ]
    }
   ],
   "source": [
    "# 1 min\n",
    "tokenized = [\n",
    "    (i, title, tokenize.sent_tokenize(preprocess_text(text)))\n",
    "    for (i, articles) in enumerate(parsed)\n",
    "    for (title, text) in articles\n",
    "]\n",
    "\n",
    "print(len(tokenized))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2286826\n",
      "2125106\n",
      "161720\n"
     ]
    }
   ],
   "source": [
    "accepted = []\n",
    "rejected = []\n",
    "\n",
    "for (collection, title, sentences) in tokenized:\n",
    "    for sentence in sentences:\n",
    "        if sentence.count(\"\\n\"):\n",
    "            rejected.append((collection, title, sentence))\n",
    "        else:\n",
    "            accepted.append((collection, title, sentence))\n",
    "\n",
    "print(len(accepted) + len(rejected))\n",
    "print(len(accepted))\n",
    "print(len(rejected))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graphs/GEC_EDA_sentences_per_collection.html'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accepted_f = Counter([collection for (collection, _, _) in accepted])\n",
    "rejected_f = Counter([collection for (collection, _, _) in rejected])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    zip(accepted_f.values(), rejected_f.values(), lens), accepted_f.keys()\n",
    ").rename(index=id_to_collection, columns={0: \"Accepted\", 1: \"Rejected\", 2: \"Articles\"})\n",
    "\n",
    "# df[\"diff\"] = abs(df[\"Accepted\"] - df[\"Rejected\"])\n",
    "fig = px.bar(\n",
    "    df,\n",
    "    barmode=\"group\",\n",
    "    log_y=True,\n",
    "    labels={\"index\": \"Collection\", \"value\": \"Sentences\"},\n",
    ")\n",
    "fig.update_xaxes(tickangle=45)\n",
    "fig.update_layout(\n",
    "    legend_title=\"\", legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.99)\n",
    ")\n",
    "\n",
    "avg = df[\"Accepted\"].mean()\n",
    "fig.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=-0.5,\n",
    "    x1=len(lens),\n",
    "    y0=avg,\n",
    "    y1=avg,\n",
    "    line=dict(color=\"Red\",),\n",
    "    xref=\"x\",\n",
    "    yref=\"y\",\n",
    ")\n",
    "# fig.show()\n",
    "\n",
    "plotly.offline.plot(fig, filename=\"graphs/GEC_EDA_sentences_per_collection.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length separation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    2125106.000000\n",
      "mean          25.175754\n",
      "std           17.586038\n",
      "min            1.000000\n",
      "25%           13.000000\n",
      "50%           22.000000\n",
      "75%           33.000000\n",
      "max          903.000000\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "accepted_lens = [\n",
    "    (collection, title, sentence, len(sentence.split(\" \")))\n",
    "    for (collection, title, sentence) in accepted\n",
    "]\n",
    "sentence_words = pd.Series([s[3] for s in accepted_lens])\n",
    "print(sentence_words.describe().apply(lambda x: format(x, \"f\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graphs/GEC_EDA_words_per_sentence.html'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = px.histogram(sentence_words, labels={\"value\": \"Words per sentence\"}, log_y=True)\n",
    "avg = np.mean(sentence_words)\n",
    "fig.add_shape(\n",
    "    type=\"line\",\n",
    "    x0=-0.5,\n",
    "    x1=max(sentence_words),\n",
    "    y0=avg,\n",
    "    y1=avg,\n",
    "    line=dict(color=\"Red\",),\n",
    "    xref=\"x\",\n",
    "    yref=\"y\",\n",
    ")\n",
    "fig.update_layout(showlegend=False, font_size=15)\n",
    "# fig.show()\n",
    "\n",
    "plotly.offline.plot(fig, filename=\"graphs/GEC_EDA_words_per_sentence.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2125106"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(accepted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short\t 475889\n",
      "Mid\t 1134569\n",
      "Long\t 514648\n"
     ]
    }
   ],
   "source": [
    "NUM_SAMPLES = 10000\n",
    "\n",
    "LOWER_THRESHOLD = 13\n",
    "UPPER_THRESHOLD = 33\n",
    "\n",
    "accepted_range = [\n",
    "    s for s in accepted_lens if s[3] >= LOWER_THRESHOLD and s[3] <= UPPER_THRESHOLD\n",
    "]\n",
    "len_lower_range = len([s for s in accepted_lens if s[3] < LOWER_THRESHOLD])\n",
    "len_upper_range = len(accepted) - len(accepted_range) - len_lower_range\n",
    "\n",
    "print(\"Short\\t\", len_lower_range)\n",
    "print(\"Mid\\t\", len(accepted_range))\n",
    "print(\"Long\\t\", len_upper_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144032\n",
      "0.5338881919301908\n",
      "0.06777638386038155\n"
     ]
    }
   ],
   "source": [
    "print(len(accepted_range) - len_lower_range - len_upper_range)\n",
    "print(len(accepted_range) / len(accepted_lens))\n",
    "print((len(accepted_range) - len_lower_range - len_upper_range) / len(accepted_lens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old version\n",
    "# df_red.to_pickle(\"samples/articles.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "final_sentences = random.sample(accepted_range, NUM_SAMPLES * 2)\n",
    "print(len(final_sentences))\n",
    "final_sentences = final_sentences[NUM_SAMPLES:]\n",
    "print(len(final_sentences))\n",
    "# with open(\"samples/3_3_sentences_20k.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(final_sentences, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0973ce1be728d4dc323b4ea761d4cc50400ec6babd1f33dbf3435fda5c4f6b44"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
